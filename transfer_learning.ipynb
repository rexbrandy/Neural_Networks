{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XcMwN4-cogr8cuL9Bnrr1NTBllyKJu2Q",
      "authorship_tag": "ABX9TyNFi8ZV5hLe+hUV3I6WHuqO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHVbR54cILVS",
        "outputId": "45d51736-13e6-4eca-ce5b-bd7dab677f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for finetuning\n",
            "Epoch: 1\n",
            "Phase: train, Loss: 0.5780056065223256, Epoch Loss: 0.6844262295081968\n",
            "Phase: val, Loss: 0.4445327759957781, Epoch Loss: 0.8562091503267973\n",
            "\n",
            "Epoch: 2\n",
            "Phase: train, Loss: 0.5315434365976052, Epoch Loss: 0.7295081967213115\n",
            "Phase: val, Loss: 0.3638048918029062, Epoch Loss: 0.9084967320261438\n",
            "\n",
            "Epoch: 3\n",
            "Phase: train, Loss: 0.4505011085115495, Epoch Loss: 0.7991803278688525\n",
            "Phase: val, Loss: 0.2901371446699878, Epoch Loss: 0.9150326797385621\n",
            "\n",
            "Epoch: 4\n",
            "Phase: train, Loss: 0.4464163724027696, Epoch Loss: 0.7909836065573771\n",
            "Phase: val, Loss: 0.25486304849581004, Epoch Loss: 0.9411764705882353\n",
            "\n",
            "Epoch: 5\n",
            "Phase: train, Loss: 0.38370965195239565, Epoch Loss: 0.8483606557377049\n",
            "Phase: val, Loss: 0.22569214912995794, Epoch Loss: 0.9411764705882353\n",
            "\n",
            "Training completed in 6m 33s\n",
            "Best val Acc: 0.9412\n",
            "Training for fixed feature extractor\n",
            "Epoch: 1\n",
            "Phase: train, Loss: 0.6215453436140155, Epoch Loss: 0.6721311475409836\n",
            "Phase: val, Loss: 0.4959676240394318, Epoch Loss: 0.7712418300653595\n",
            "\n",
            "Epoch: 2\n",
            "Phase: train, Loss: 0.504145698957756, Epoch Loss: 0.7745901639344263\n",
            "Phase: val, Loss: 0.45438191869290045, Epoch Loss: 0.7973856209150327\n",
            "\n",
            "Epoch: 3\n",
            "Phase: train, Loss: 0.5220594359714477, Epoch Loss: 0.7622950819672131\n",
            "Phase: val, Loss: 0.3428575574767356, Epoch Loss: 0.8888888888888888\n",
            "\n",
            "Epoch: 4\n",
            "Phase: train, Loss: 0.46657358232091684, Epoch Loss: 0.819672131147541\n",
            "Phase: val, Loss: 0.29711180899803546, Epoch Loss: 0.9281045751633987\n",
            "\n",
            "Epoch: 5\n",
            "Phase: train, Loss: 0.4219186726163645, Epoch Loss: 0.8032786885245902\n",
            "Phase: val, Loss: 0.27265236414725486, Epoch Loss: 0.9150326797385621\n",
            "\n",
            "Training completed in 3m 17s\n",
            "Best val Acc: 0.9281\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "#\n",
        "# Declare everything\n",
        "# \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Datasets are images stored in `train/` and `val/` folders\n",
        "data_dir = '/content/drive/MyDrive/neural_network_data/hymenoptera_data'\n",
        "\n",
        "sets = ['train', 'val'] # Names of each dataset\n",
        "\n",
        "image_datasets = { x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) \n",
        "                for x in sets}\n",
        "\n",
        "dataloaders = { x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) \n",
        "                for x in sets}\n",
        "\n",
        "dataset_sizes = { x: len(image_datasets[x]) for x in sets} # dictionary that holds length of each data set (train, val)\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "#\n",
        "# Training loop\n",
        "#\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    '''\n",
        "        Basic training loop.\n",
        "\n",
        "        Params:\n",
        "        model - NN model, holds layers and forward pass function\n",
        "        criterion - loss function, handles backprop and calculates loss\n",
        "        optimizer - optimization algorithm, updates params and optim steps\n",
        "        scheduler - updates learning rate as our loss changes\n",
        "        num_epochs - how many train/eval loops we should do\n",
        "    '''\n",
        "\n",
        "    # Track how long training took\n",
        "    since = time.time()\n",
        "\n",
        "    # We will save the best model state in case we move away from it\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Begin training loops\n",
        "        print(f'Epoch: {epoch+1}')\n",
        "\n",
        "        # The training phase will go first\n",
        "        # then an evaluation phase\n",
        "        # then epoch will end\n",
        "        for phase in sets:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            elif phase == 'val':\n",
        "                model.eval()\n",
        "        \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            \n",
        "            # Use dataloader to split dataset into batches\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                # track track gradients if train phase\n",
        "                with torch.set_grad_enabled(phase == 'train'):           \n",
        "                    output = model(inputs)\n",
        "                    _, preds = torch.max(output, 1)\n",
        "                    loss = criterion(output, labels)\n",
        "\n",
        "                    # Backward pass and optim\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # If it is the end of the training phase update learning rate\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'Phase: {phase}, Loss: {epoch_loss}, Epoch Loss: {epoch_acc}')\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                '''\n",
        "                    If it is the training phase \n",
        "                    AND the accuracy from this epoch is better than the currect best\n",
        "\n",
        "                    Update best accuracy\n",
        "                    Copy model `state_dict`\n",
        "                '''\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training completed in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "#\n",
        "# Transfer learning\n",
        "#\n",
        "\n",
        "# Finetuning the Convnet\n",
        "# Load the pretrained model and reset the fully connected layer\n",
        "# When gradients are calculated weights are tuned through out the whole network\n",
        "# This is finetuning\n",
        "finetune_model = models.resnet18(pretrained=True)\n",
        "\n",
        "num_features = finetune_model.fc.in_features \n",
        "# reassign model.fc || output size is 2 (ants, bees)\n",
        "finetune_model.fc = nn.Linear(in_features=num_features, out_features=2) \n",
        "\n",
        "finetune_model.to(device)\n",
        "\n",
        "# Criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(finetune_model.parameters(), lr=0.001)\n",
        "\n",
        "# Step scheduler \n",
        "# every 7 epochs lr=lr*gamma\n",
        "step_lr_scheduler = lr_scheduler.StepLR( \n",
        "    optimizer=optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "print('Training for finetuning')\n",
        "finetune_model = train_model(\n",
        "    finetune_model, criterion, optimizer, scheduler=step_lr_scheduler, num_epochs=5)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "# Fixed feature extractor model\n",
        "# Freeze gradients in ConvNet so that only the \n",
        "# fully connected layer has gradients computed during backprop\n",
        "ff_model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze ConvNet\n",
        "for param in ff_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = ff_model.fc.in_features \n",
        "# reassign model.fc || output size is 2 (ants, bees)\n",
        "ff_model.fc = nn.Linear(in_features=num_features, out_features=2) \n",
        "\n",
        "ff_model.to(device)\n",
        "\n",
        "# Criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(ff_model.parameters(), lr=0.001)\n",
        "\n",
        "# Step scheduler \n",
        "# every 7 epochs lr=lr*gamma\n",
        "step_lr_scheduler = lr_scheduler.StepLR( \n",
        "    optimizer=optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "print('Training for fixed feature extractor')\n",
        "ff_model = train_model(\n",
        "    ff_model, criterion, optimizer, scheduler=step_lr_scheduler, num_epochs=5)\n",
        "\n"
      ]
    }
  ]
}